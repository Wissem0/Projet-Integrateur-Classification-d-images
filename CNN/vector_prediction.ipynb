{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pylab as plt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_folder = \"C:/Users/Rostom/Videos/5SDBD/Projet_Int√©grateur/archive/\"\n",
    "ATTR_PATH = main_folder + \"list_attr_celeba.csv\"  \n",
    "PARTITION_PATH = main_folder + \"list_eval_partition.csv\" \n",
    "IMAGES_PATH = main_folder + \"img_align_celeba/img_align_celeba/\"  \n",
    "MODEL_HANDLE = \"./model\" \n",
    "IMAGE_SIZE = (220, 220)\n",
    "\n",
    "# Performance\n",
    "# BATCH_SIZE = 128\n",
    "BATCH_SIZE = 150\n",
    "TRAIN_SAMPLE = 20000\n",
    "VALIDATION_SAMPLE = 4000\n",
    "NUM_EPOCHS = 3\n",
    "# NUM_EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mouth_Slightly_Open',\n",
       " 'Smiling',\n",
       " 'Wearing_Lipstick',\n",
       " 'High_Cheekbones',\n",
       " 'Male',\n",
       " 'Heavy_Makeup',\n",
       " 'Wavy_Hair',\n",
       " 'Oval_Face',\n",
       " 'Pointy_Nose',\n",
       " 'Arched_Eyebrows',\n",
       " 'Big_Lips',\n",
       " 'Black_Hair',\n",
       " 'Big_Nose',\n",
       " 'Young',\n",
       " 'Straight_Hair',\n",
       " 'Brown_Hair',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Wearing_Earrings',\n",
       " 'No_Beard',\n",
       " 'Bangs',\n",
       " 'Blond_Hair',\n",
       " 'Chubby',\n",
       " 'Bald']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def select_best_features():\n",
    "    df1 = pd.read_csv(ATTR_PATH, delimiter=',')\n",
    "    df1.dataframeName = 'list_attr_celeba.csv'\n",
    "    scores=[]\n",
    "    for column in df1:\n",
    "        if column != 'image_id':\n",
    "            a = len(df1[df1[column] == 1])\n",
    "            b = len(df1[df1[column] == -1])\n",
    "            score = abs(a-b)/(a+b)\n",
    "            scores.append((column,1/score))\n",
    "    scores.sort(reverse = True)\n",
    "    scores.sort(key=lambda a: a[1])\n",
    "    scores.reverse()\n",
    "    N=22\n",
    "    scores_subset = [x for index, x in enumerate(scores) if index < N]\n",
    "    selected_features = [scores_subset[i][0] for i in range(len(scores_subset))]\n",
    "    \n",
    "    #Manual tweaking\n",
    "    selected_features.remove('Attractive')\n",
    "    selected_features.append('Chubby')\n",
    "    selected_features.append('Bald')\n",
    "    return selected_features\n",
    "\n",
    "select_best_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_id\n",
      "partition\n",
      "5_o_Clock_Shadow\n",
      "Arched_Eyebrows\n",
      "Attractive\n",
      "Bags_Under_Eyes\n",
      "Bald\n",
      "Bangs\n",
      "Big_Lips\n",
      "Big_Nose\n",
      "Black_Hair\n",
      "Blond_Hair\n",
      "Blurry\n",
      "Brown_Hair\n",
      "Bushy_Eyebrows\n",
      "Chubby\n",
      "Double_Chin\n",
      "Eyeglasses\n",
      "Goatee\n",
      "Gray_Hair\n",
      "Heavy_Makeup\n",
      "High_Cheekbones\n",
      "Male\n",
      "Mouth_Slightly_Open\n",
      "Mustache\n",
      "Narrow_Eyes\n",
      "No_Beard\n",
      "Oval_Face\n",
      "Pale_Skin\n",
      "Pointy_Nose\n",
      "Receding_Hairline\n",
      "Rosy_Cheeks\n",
      "Sideburns\n",
      "Smiling\n",
      "Straight_Hair\n",
      "Wavy_Hair\n",
      "Wearing_Earrings\n",
      "Wearing_Hat\n",
      "Wearing_Lipstick\n",
      "Wearing_Necklace\n",
      "Wearing_Necktie\n",
      "Young\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(pd.read_csv(PARTITION_PATH), pd.read_csv(ATTR_PATH), on=\"image_id\")\n",
    "df.head()\n",
    "for column in df:\n",
    "    print(column)\n",
    "features_str = select_best_features()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMAGE_SIZE)\n",
    "    return image\n",
    "\n",
    "def load_and_preprocess_image(path):\n",
    "    image = tf.io.read_file(path)\n",
    "    return preprocess_image(image)\n",
    "\n",
    "def load_and_preprocess_from_path_label(path, \n",
    "    Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald):\n",
    "    images = load_and_preprocess_image(path)\n",
    "    return images, Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose,Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald\n",
    "\n",
    "def init_dataset(df,features_str):\n",
    "    a = ([IMAGES_PATH + image_id for image_id in df[\"image_id\"]],)\n",
    "    for feature in features_str:\n",
    "        b =  list(df[feature].replace(-1,0))\n",
    "        a = a + (b,)\n",
    "    return a\n",
    "\n",
    "\n",
    "def build_dataset_from_df(df):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(init_dataset(df,features_str))\n",
    "    ds = ds.map(load_and_preprocess_from_path_label)\n",
    "    ds = ds.shuffle(buffer_size=1000)\n",
    "    ds = ds.repeat()\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.loc[df[\"partition\"] == 0].head(TRAIN_SAMPLE)\n",
    "train_ds = build_dataset_from_df(train_df)\n",
    "\n",
    "val_df = df.loc[df[\"partition\"] == 1].head(VALIDATION_SAMPLE)\n",
    "val_ds = build_dataset_from_df(val_df)\n",
    "\n",
    "#val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald = next(iter(train_ds))\n",
    "features = [ Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald]\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(image[i].numpy().astype(\"uint8\"))\n",
    "    s = \"\"\n",
    "    for j in range(len(features)):\n",
    "        s+= str(features_str[j]) +\" : \"\n",
    "        s+= str(features[j][i].numpy())\n",
    "        s+= '\\n'\n",
    "#     plt.title(s)\n",
    "#     plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\autograph\\pyct\\static_analysis\\liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
    "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
    "do_data_augmentation = False\n",
    "if do_data_augmentation:\n",
    "    preprocessing_model.add(tf.keras.layers.RandomRotation(0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomTranslation(0, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomTranslation(0.2, 0))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomZoom(0.2, 0.2))\n",
    "    preprocessing_model.add(tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
    "train_ds = train_ds.map(lambda images,  Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald:\n",
    "                        (preprocessing_model(images), ( Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald)))\n",
    "\n",
    "val_ds = val_ds.map(lambda images,  Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald:\n",
    "                    (normalization_layer(images), ( Mouth_Slightly_Open, Smiling, Wearing_Lipstick, High_Cheekbones, Male, Heavy_Makeup, Wavy_Hair, Oval_Face, Pointy_Nose, Arched_Eyebrows, Big_Lips, Black_Hair, Big_Nose, Young, Straight_Hair, Brown_Hair, Bags_Under_Eyes, Wearing_Earrings, No_Beard, Bangs, Blond_Hair, Chubby, Bald)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please fix your imports. Module tensorflow.python.training.tracking.data_structures has been moved to tensorflow.python.trackable.data_structures. The old module will be deleted in version 2.11.\n"
     ]
    }
   ],
   "source": [
    "do_fine_tuning = False\n",
    "input = tf.keras.Input(shape=IMAGE_SIZE + (3,))\n",
    "x = hub.KerasLayer(MODEL_HANDLE, trainable=do_fine_tuning)(input)\n",
    "x = tf.keras.layers.Dropout(rate=0.2)(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "\n",
    "outs = []\n",
    "\n",
    "for feature in features_str:\n",
    "    out = tf.keras.layers.Dense(1, kernel_regularizer=tf.keras.regularizers.l2(0.0001), activation=\"sigmoid\", name=feature)(x)\n",
    "    outs.append(out)\n",
    "\n",
    "\n",
    "model = tf.keras.Model( inputs = input, outputs = outs)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss = {           \n",
    "        \"Mouth_Slightly_Open\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Smiling\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Wearing_Lipstick\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"High_Cheekbones\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Male\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Heavy_Makeup\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Wavy_Hair\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Oval_Face\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Pointy_Nose\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Arched_Eyebrows\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Big_Lips\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Black_Hair\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Big_Nose\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Young\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Straight_Hair\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Brown_Hair\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Bags_Under_Eyes\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Wearing_Earrings\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"No_Beard\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Bangs\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Blond_Hair\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Chubby\": tf.keras.losses.BinaryCrossentropy(),\n",
    "        \"Bald\": tf.keras.losses.BinaryCrossentropy() \n",
    "    },\n",
    "    metrics = {\n",
    "        \"Mouth_Slightly_Open\": 'accuracy',\n",
    "        \"Smiling\": 'accuracy',\n",
    "        \"Wearing_Lipstick\": 'accuracy',\n",
    "        \"High_Cheekbones\": 'accuracy',\n",
    "        \"Male\": 'accuracy',\n",
    "        \"Heavy_Makeup\": 'accuracy',\n",
    "        \"Wavy_Hair\": 'accuracy',\n",
    "        \"Oval_Face\": 'accuracy',\n",
    "        \"Pointy_Nose\": 'accuracy',\n",
    "        \"Arched_Eyebrows\": 'accuracy',\n",
    "        \"Big_Lips\": 'accuracy',\n",
    "        \"Black_Hair\": 'accuracy',\n",
    "        \"Big_Nose\": 'accuracy',\n",
    "        \"Young\": 'accuracy',\n",
    "        \"Straight_Hair\": 'accuracy',\n",
    "        \"Brown_Hair\": 'accuracy',\n",
    "        \"Bags_Under_Eyes\": 'accuracy',\n",
    "        \"Wearing_Earrings\": 'accuracy',\n",
    "        \"No_Beard\": 'accuracy',\n",
    "        \"Bangs\": 'accuracy',\n",
    "        \"Blond_Hair\": 'accuracy',\n",
    "        \"Chubby\": 'accuracy',\n",
    "        \"Bald\": 'accuracy'\n",
    "    },\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  7/133 [>.............................] - ETA: 7:58 - loss: 15.0241 - Mouth_Slightly_Open_loss: 0.8762 - Smiling_loss: 0.7570 - Wearing_Lipstick_loss: 0.8619 - High_Cheekbones_loss: 0.7192 - Male_loss: 0.6146 - Heavy_Makeup_loss: 0.6578 - Wavy_Hair_loss: 0.7311 - Oval_Face_loss: 0.6716 - Pointy_Nose_loss: 0.6214 - Arched_Eyebrows_loss: 0.6836 - Big_Lips_loss: 0.5917 - Black_Hair_loss: 0.7262 - Big_Nose_loss: 0.7367 - Young_loss: 0.7114 - Straight_Hair_loss: 0.6074 - Brown_Hair_loss: 0.5743 - Bags_Under_Eyes_loss: 0.6762 - Wearing_Earrings_loss: 0.6374 - No_Beard_loss: 0.8123 - Bangs_loss: 0.5108 - Blond_Hair_loss: 0.4325 - Chubby_loss: 0.3410 - Bald_loss: 0.4671 - Mouth_Slightly_Open_accuracy: 0.5048 - Smiling_accuracy: 0.5038 - Wearing_Lipstick_accuracy: 0.5257 - High_Cheekbones_accuracy: 0.5276 - Male_accuracy: 0.6867 - Heavy_Makeup_accuracy: 0.6314 - Wavy_Hair_accuracy: 0.6238 - Oval_Face_accuracy: 0.6514 - Pointy_Nose_accuracy: 0.6695 - Arched_Eyebrows_accuracy: 0.5838 - Big_Lips_accuracy: 0.7400 - Black_Hair_accuracy: 0.5762 - Big_Nose_accuracy: 0.6029 - Young_accuracy: 0.6029 - Straight_Hair_accuracy: 0.7143 - Brown_Hair_accuracy: 0.7686 - Bags_Under_Eyes_accuracy: 0.6029 - Wearing_Earrings_accuracy: 0.7124 - No_Beard_accuracy: 0.5724 - Bangs_accuracy: 0.8048 - Blond_Hair_accuracy: 0.8400 - Chubby_accuracy: 0.8733 - Bald_accuracy: 0.8000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-e49b77097176>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msteps_per_epoch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvalidation_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_df\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m hist = model.fit(\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtrain_ds\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1648\u001b[0m                         ):\n\u001b[0;32m   1649\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1650\u001b[1;33m                             \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1651\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1652\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    132\u001b[0m       (concrete_function,\n\u001b[0;32m    133\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m--> 134\u001b[1;33m     return concrete_function._call_flat(\n\u001b[0m\u001b[0;32m    135\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    376\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    379\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Rostom\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "validation_steps = len(val_df) // BATCH_SIZE\n",
    "hist = model.fit(\n",
    "    train_ds,\n",
    "    epochs=NUM_EPOCHS, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds,\n",
    "    validation_steps=validation_steps).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(features_str), 2, figsize=(35, 35))\n",
    "for i, c in enumerate(features_str):\n",
    "    ax[i, 0].plot(hist[f\"{c}_loss\"], label=\"train\")\n",
    "    ax[i, 0].plot(hist[f\"val_{c}_loss\"], label=\"val\")\n",
    "    ax[i, 0].set_title(f\"Loss ({c})\")\n",
    "    ax[i, 0].legend()\n",
    "    ax[i, 1].plot(hist[f\"{c}_accuracy\"], label=\"train\")\n",
    "    ax[i, 1].plot(hist[f\"val_{c}_accuracy\"], label=\"val\")\n",
    "    ax[i, 1].set_title(f\"Accuracy ({c})\")\n",
    "    ax[i, 1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_ds))\n",
    "image = x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(val_ds))\n",
    "image = x[0, :, :, :]\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "#print(x)\n",
    "\n",
    "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
    "for i, label in enumerate(features_str):\n",
    "    pred = prediction_scores[i][0][0]\n",
    "    print(f\"{label}: actual {y[i][0]}, predicted {1 if pred > 0.5 else 0} ({format(pred, '.4f')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_images(inference_folder: str) -> np.ndarray:\n",
    "    images = []\n",
    "    for img in os.listdir(inference_folder):\n",
    "        img_location = os.path.join(inference_folder, img)  \n",
    "        with Image.open(img_location) as img:  \n",
    "            img = np.array(img)\n",
    "            img = img[:, :, :3]\n",
    "            img = np.expand_dims(img, axis=0)  \n",
    "        images.append(img)\n",
    "    images_array = np.vstack(images) \n",
    "    return images_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "path = \"./image_samples/\"\n",
    "dirs = os.listdir( path )\n",
    "resized_path = \"./images_resized/\"\n",
    "\n",
    "def resize():\n",
    "    for item in dirs:\n",
    "        print(item)\n",
    "        if os.path.isfile(path+item):\n",
    "            im = Image.open(path+item)\n",
    "            im = im.convert('RGB')\n",
    "            f, e = os.path.splitext(path+item)\n",
    "            imResize = im.resize((220,220), Image.ANTIALIAS)\n",
    "            imResize.save(resized_path + item + '_resized.jpg', 'JPEG', quality=90)\n",
    "\n",
    "resize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image,s):\n",
    "    plt.imshow(image)\n",
    "    plt.title(s)\n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_prediction(prediction_score):\n",
    "    s = \"\"\n",
    "    for i, label in enumerate(features_str):\n",
    "        pred = prediction_score[i][0][0]\n",
    "        s += f\"{label}: predicted {1 if pred > 0.5 else 0} ({format(pred, '.4f')})\"\n",
    "        s +=\"\\n\"\n",
    "    return s  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "images = open_images(resized_path)\n",
    "c=0\n",
    "for image in images:\n",
    "    #img_batch = np.expand_dims(image * 10e-4, axis=0)\n",
    "    img_batch = np.expand_dims(image, axis=0)\n",
    "    #print(img_batch)\n",
    "    normalizedData = (img_batch-np.min(img_batch))/(np.max(img_batch)-np.min(img_batch))\n",
    "    #print(normalizedData)\n",
    "    prediction_score= model.predict(normalizedData)\n",
    "    s = image_prediction(prediction_score)\n",
    "    display_image(image,s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "153a4f8d753e887c3d75f202e37c12a50607ecb8a69b9559b64e798fc62fe837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
